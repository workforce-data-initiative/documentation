{
    "docs": [
        {
            "location": "/", 
            "text": "DataAtWork Documentation\n\n\nTechnical documentation for DataAtWork, including detailed documentation of the\n\nOpen Skills Project\n and the \nTraining Provider Outcomes Toolkit\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#dataatwork-documentation", 
            "text": "Technical documentation for DataAtWork, including detailed documentation of the Open Skills Project  and the  Training Provider Outcomes Toolkit .", 
            "title": "DataAtWork Documentation"
        }, 
        {
            "location": "/open-skills/", 
            "text": "Open Skills Project\n\n\nThe Open Skills Project is focused on providing a dynamic, up-to-date, locally-relevant, and normalized taxonomy of skills and jobs that builds on and expands on the Department of Labor\u2019s O*NET data resources. This taxonomy is aimed at two groups:\n\n\n\n\nDevelopers, by way of the \nOpen Skills API\n\n\nResearchers, using tabular data sets (coming soon!)\n\n\n\n\nCollecting Data\n\n\nInformation used to create the Open Skills API comes from a variety of sources:\n    - Public and private providers of job listings\n    - O*NET jobs and skills taxonomy (https://www.onetonline.org/)\n\n\nThe job listings data sources are converted into a common job listing format, based on the schema.org Job Posting Schema (https://schema.org/JobPosting), and saved as JSON into an S3 folder according to the quarter(s) in which they are active.\n\n\nProcessing\n\n\nONET taxonomy data is transformed into master tables of jobs and skills, and associations between jobs and skills.\nJob posting titles are cleaned, aggregated into geographical counts. The titles and descriptions are indexed into Elasticsearch to implement a rudimentary job title normalizer.\n\n\nOutput\n\n\nA tabular version of each processed data set is uploaded to a publicly accessible S3 bucket for use by researchers. The processed data is also loaded into a relational database, which the Open Skills API queries to retrieve data in response to user requests.\n\n\nCode\n\n\nTo produce this output, a variety of extraction and processing tasks are used across four different code repositories.\n\n\n\n\nskills-utils\n contains common utilities regarding things like hashing, Elasticsearch, S3, to be used by different parts of the Open Skills Project.\n\n\nskills-ml\n contains processing algorithms and integrations with various open datasets to help compute our jobs and skills taxonomy.\n\n\nskills-airflow\n contains an orchestration workflow using the Airflow project that combines tasks from our skills-public-etl and skills-ml tasks to create aggregated data suitable for public consumption.\n\n\nskills-api\n contains a Flask application that runs the Open Skills API, making available data generated by the skills-airflow repository for developer use.", 
            "title": "Open Skills"
        }, 
        {
            "location": "/open-skills/#open-skills-project", 
            "text": "The Open Skills Project is focused on providing a dynamic, up-to-date, locally-relevant, and normalized taxonomy of skills and jobs that builds on and expands on the Department of Labor\u2019s O*NET data resources. This taxonomy is aimed at two groups:   Developers, by way of the  Open Skills API  Researchers, using tabular data sets (coming soon!)", 
            "title": "Open Skills Project"
        }, 
        {
            "location": "/open-skills/#collecting-data", 
            "text": "Information used to create the Open Skills API comes from a variety of sources:\n    - Public and private providers of job listings\n    - O*NET jobs and skills taxonomy (https://www.onetonline.org/)  The job listings data sources are converted into a common job listing format, based on the schema.org Job Posting Schema (https://schema.org/JobPosting), and saved as JSON into an S3 folder according to the quarter(s) in which they are active.", 
            "title": "Collecting Data"
        }, 
        {
            "location": "/open-skills/#processing", 
            "text": "ONET taxonomy data is transformed into master tables of jobs and skills, and associations between jobs and skills.\nJob posting titles are cleaned, aggregated into geographical counts. The titles and descriptions are indexed into Elasticsearch to implement a rudimentary job title normalizer.", 
            "title": "Processing"
        }, 
        {
            "location": "/open-skills/#output", 
            "text": "A tabular version of each processed data set is uploaded to a publicly accessible S3 bucket for use by researchers. The processed data is also loaded into a relational database, which the Open Skills API queries to retrieve data in response to user requests.", 
            "title": "Output"
        }, 
        {
            "location": "/open-skills/#code", 
            "text": "To produce this output, a variety of extraction and processing tasks are used across four different code repositories.   skills-utils  contains common utilities regarding things like hashing, Elasticsearch, S3, to be used by different parts of the Open Skills Project.  skills-ml  contains processing algorithms and integrations with various open datasets to help compute our jobs and skills taxonomy.  skills-airflow  contains an orchestration workflow using the Airflow project that combines tasks from our skills-public-etl and skills-ml tasks to create aggregated data suitable for public consumption.  skills-api  contains a Flask application that runs the Open Skills API, making available data generated by the skills-airflow repository for developer use.", 
            "title": "Code"
        }, 
        {
            "location": "/tpot/", 
            "text": "Training Provider Outcomes Toolkit\n\n\nThe Training Provider Outcomes Toolkit (TPOT) is a collection of tools for\nsecurely collecting, connecting, analyzing, aggregating, and publishing data on\nwage and employment outcomes for education and training participants.\n\n\nState and local workforce development boards aim to improve employment and\nworkforce investment in their regions.\n\n\nCollecting data from training providers\n\n\nThere are many hundreds or thousands of training providers within the purview\nof each workforce development board. Each one must securely upload their\nparticipant data to their workforce board in order to be eligible for federal\nfunds. The resulting aggregate statistics on participant outcomes will be\ninvaluable marketing materials for successful programs, regardless of federal\nfunding. This means that the workforce development boards must be equipped to\nreceive and validate the data.\n\n\nTraining providers range from small trade apprenticeships to community colleges\nto multi-state organizations, with a wide range of data sophistication. The\nway(s) in which the workforce data board collects participant outcomes must be\neasy and accessible to all organizations. At the same time, it must be easy\nfor the board itself to automatically process and validate the datasets.\n\n\nThe Data Package\n\n\nIn order to support these use-cases, the toolkit uses the \nData Package\n\nspecification in order to encapsulate and describe training provider outcome\ndata in a systematic fashion.\n\n\nCreate a Data Package specification\n\n\nEach workforce development board should create a data package specification\nthat defines the specific fields and values that they require the training\nproviders to use.\n\n\nThe \nData Packagist\n tool allows you to\neasily define these packages in the browser. You can also view existing package\nspecifications with the \nonline data package\nviewer\n.  Finally, you can validate your\nspecification with an \nonline validator\n.\n\n\nUploading website\n\n\nOnce a data package specification is created, it may be used to easily\ncustomize a self-hosted website that enables validating and uploading data\npackages from training providers. This site is designed to be simple to modify\nand setup a custom instance for each training provider. A demo uploading\nwebsite is available at \nsend.dataatwork.org\n. The\nsource code is available for customization \non\nGitHub\n.\n\n\nThe data warehouse\n\n\nThe upload website is designed to securely upload the data packages to a\ndata warehouse for each workforce board.  \nStay tuned for updates.\n\n\nData matching\n\n\nOnce the workforce board has collected the training providers' outcomes, it\nmust link the participant information with wage and employment information from\nother departments. In order to facilitate this matching, we have developed an\neasy-to-use and open-source deduplication package,\n\nSuperDeduper\n. This allows for a\ncombination of exact record linkage and probabalistic fuzzy matching in order\nto link as many participants as possible to the other datasets.\n\n\nData processing\n\n\nA \nwide variety of tools\n have been\ndeveloped that support the Data Package format. It is possible to load your\ndata package into \nmany analysis\nlanguages\n including\n\nPython\n and\n\nR\n. It's also possible to\nload the data packages into \nmany database\nsystems\n including\n\nSQL Server\n and\n\nPostgreSQL\n\n . Work is underway to support\n\nExcel\n and \nGoogle\nSheets\n.\n\n\nData export\n\n\nOnce the individual outcome data has been aggregated into ETP scorecards for\neach program, the data must be publicly accessible for others to analyze and\nbuild upon. We are in the process of developing an API that can serve these\nscorecards in a machine-readable manner that will allow for development of\nmore sophisticated tools and analyses.  We are developing a framework to\nallow easy deployment of a web service for this API at \nGitHub: etp-api\n. An alpha version is\navailable with sample data at \netp-api.dataatwork.org\n.", 
            "title": "TPOT"
        }, 
        {
            "location": "/tpot/#training-provider-outcomes-toolkit", 
            "text": "The Training Provider Outcomes Toolkit (TPOT) is a collection of tools for\nsecurely collecting, connecting, analyzing, aggregating, and publishing data on\nwage and employment outcomes for education and training participants.  State and local workforce development boards aim to improve employment and\nworkforce investment in their regions.", 
            "title": "Training Provider Outcomes Toolkit"
        }, 
        {
            "location": "/tpot/#collecting-data-from-training-providers", 
            "text": "There are many hundreds or thousands of training providers within the purview\nof each workforce development board. Each one must securely upload their\nparticipant data to their workforce board in order to be eligible for federal\nfunds. The resulting aggregate statistics on participant outcomes will be\ninvaluable marketing materials for successful programs, regardless of federal\nfunding. This means that the workforce development boards must be equipped to\nreceive and validate the data.  Training providers range from small trade apprenticeships to community colleges\nto multi-state organizations, with a wide range of data sophistication. The\nway(s) in which the workforce data board collects participant outcomes must be\neasy and accessible to all organizations. At the same time, it must be easy\nfor the board itself to automatically process and validate the datasets.", 
            "title": "Collecting data from training providers"
        }, 
        {
            "location": "/tpot/#the-data-package", 
            "text": "In order to support these use-cases, the toolkit uses the  Data Package \nspecification in order to encapsulate and describe training provider outcome\ndata in a systematic fashion.", 
            "title": "The Data Package"
        }, 
        {
            "location": "/tpot/#create-a-data-package-specification", 
            "text": "Each workforce development board should create a data package specification\nthat defines the specific fields and values that they require the training\nproviders to use.  The  Data Packagist  tool allows you to\neasily define these packages in the browser. You can also view existing package\nspecifications with the  online data package\nviewer .  Finally, you can validate your\nspecification with an  online validator .", 
            "title": "Create a Data Package specification"
        }, 
        {
            "location": "/tpot/#uploading-website", 
            "text": "Once a data package specification is created, it may be used to easily\ncustomize a self-hosted website that enables validating and uploading data\npackages from training providers. This site is designed to be simple to modify\nand setup a custom instance for each training provider. A demo uploading\nwebsite is available at  send.dataatwork.org . The\nsource code is available for customization  on\nGitHub .", 
            "title": "Uploading website"
        }, 
        {
            "location": "/tpot/#the-data-warehouse", 
            "text": "The upload website is designed to securely upload the data packages to a\ndata warehouse for each workforce board.   Stay tuned for updates.", 
            "title": "The data warehouse"
        }, 
        {
            "location": "/tpot/#data-matching", 
            "text": "Once the workforce board has collected the training providers' outcomes, it\nmust link the participant information with wage and employment information from\nother departments. In order to facilitate this matching, we have developed an\neasy-to-use and open-source deduplication package, SuperDeduper . This allows for a\ncombination of exact record linkage and probabalistic fuzzy matching in order\nto link as many participants as possible to the other datasets.", 
            "title": "Data matching"
        }, 
        {
            "location": "/tpot/#data-processing", 
            "text": "A  wide variety of tools  have been\ndeveloped that support the Data Package format. It is possible to load your\ndata package into  many analysis\nlanguages  including Python  and R . It's also possible to\nload the data packages into  many database\nsystems  including SQL Server  and PostgreSQL \n . Work is underway to support Excel  and  Google\nSheets .", 
            "title": "Data processing"
        }, 
        {
            "location": "/tpot/#data-export", 
            "text": "Once the individual outcome data has been aggregated into ETP scorecards for\neach program, the data must be publicly accessible for others to analyze and\nbuild upon. We are in the process of developing an API that can serve these\nscorecards in a machine-readable manner that will allow for development of\nmore sophisticated tools and analyses.  We are developing a framework to\nallow easy deployment of a web service for this API at  GitHub: etp-api . An alpha version is\navailable with sample data at  etp-api.dataatwork.org .", 
            "title": "Data export"
        }
    ]
}